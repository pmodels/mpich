# vim: set ft=c:

MPI_Allgather:
    .desc: Gathers data from all tasks and distribute the combined
/*
    Notes:
     The MPI standard (1.0 and 1.1) says that
    .n
    .n
     The jth block of data sent from  each process is received by every process
     and placed in the jth block of the buffer 'recvbuf'.
    .n
    .n
     This is misleading; a better description is
    .n
    .n
     The block of data sent from the jth process is received by every
     process and placed in the jth block of the buffer 'recvbuf'.
    .n
    .n
     This text was suggested by Rajeev Thakur and has been adopted as a
     clarification by the MPI Forum.
*/

MPI_Allgatherv:
    .desc: Gathers data from all tasks and deliver the combined data
/*
    Notes:
     The MPI standard (1.0 and 1.1) says that
    .n
    .n
     The jth block of data sent from
     each process is received by every process and placed in the jth block of the
     buffer 'recvbuf'.
    .n
    .n
     This is misleading; a better description is
    .n
    .n
     The block of data sent from the jth process is received by every
     process and placed in the jth block of the buffer 'recvbuf'.
    .n
    .n
     This text was suggested by Rajeev Thakur, and has been adopted as a
     clarification to the MPI standard by the MPI-Forum.
*/

MPI_Allreduce:
    .desc: Combines values from all processes and distributes the result
    .extra: collops

MPI_Alltoall:
    .desc: Sends data from all to all processes

MPI_Alltoallv:
    .desc: Sends data from all to all processes; each process may

MPI_Alltoallw:
    .desc: Generalized all-to-all communication allowing different

MPI_Barrier:
    .desc: Blocks until all processes in the communicator have
/*
    Notes:
    Blocks the caller until all processes in the communicator have called it;
    that is, the call returns at any process only after all members of the
    communicator have entered the call.
*/

MPI_Bcast:
    .desc: Broadcasts a message from the process with rank "root" to

MPI_Exscan:
    .desc: Computes the exclusive scan (partial reductions) of data on a
    .extra: collops, errtest_comm_intra
/*
    Notes:
      'MPI_Exscan' is like 'MPI_Scan', except that the contribution from the
       calling process is not included in the result at the calling process
       (it is contributed to the subsequent processes, of course).
*/

MPI_Gather:
    .desc: Gathers together values from a group of processes

MPI_Gatherv:
    .desc: Gathers into specified locations from all processes in a group

MPI_Iallgather:
    .desc: Gathers data from all tasks and distribute the combined data

MPI_Iallgatherv:
    .desc: Gathers data from all tasks and deliver the combined data

MPI_Iallreduce:
    .desc: Combines values from all processes and distributes the result

MPI_Ialltoall:
    .desc: Sends data from all to all processes in a nonblocking way

MPI_Ialltoallv:
    .desc: Sends data from all to all processes in a nonblocking way;

MPI_Ialltoallw:
    .desc: Nonblocking generalized all-to-all communication allowing

MPI_Ibarrier:
    .desc: Notifies the process that it has reached the barrier and returns
/*
    Notes:
    MPI_Ibarrier is a nonblocking version of MPI_barrier. By calling MPI_Ibarrier,
    a process notifies that it has reached the barrier. The call returns
    immediately, independent of whether other processes have called MPI_Ibarrier.
    The usual barrier semantics are enforced at the corresponding completion
    operation (test or wait), which in the intra-communicator case will complete
    only after all other processes in the communicator have called MPI_Ibarrier. In
    the intercommunicator case, it will complete when all processes in the remote
    group have called MPI_Ibarrier.
*/

MPI_Ibcast:
    .desc: Broadcasts a message from the process with rank "root" to

MPI_Iexscan:
    .desc: Computes the exclusive scan (partial reductions) of data on a
    .extra: collops, errtest_comm_intra

MPI_Igather:
    .desc: Gathers together values from a group of processes in

MPI_Igatherv:
    .desc: Gathers into specified locations from all processes in a group

MPI_Ineighbor_allgather:
    .desc: Nonblocking version of MPI_Neighbor_allgather.

MPI_Ineighbor_allgatherv:
    .desc: Nonblocking version of MPI_Neighbor_allgatherv.

MPI_Ineighbor_alltoall:
    .desc: Nonblocking version of MPI_Neighbor_alltoall.

MPI_Ineighbor_alltoallv:
    .desc: Nonblocking version of MPI_Neighbor_alltoallv.

MPI_Ineighbor_alltoallw:
    .desc: Nonblocking version of MPI_Neighbor_alltoallw.

MPI_Ireduce:
    .desc: Reduces values on all processes to a single value

MPI_Ireduce_scatter:
    .desc: Combines values and scatters the results in

MPI_Ireduce_scatter_block:
    .desc: Combines values and scatters the results in

MPI_Iscan:
    .desc: Computes the scan (partial reductions) of data on a collection of
    .extra: collops, errtest_comm_intra

MPI_Iscatter:
    .desc: Sends data from one process to all other processes in a

MPI_Iscatterv:
    .desc: Scatters a buffer in parts to all processes in a communicator

MPI_Neighbor_allgather:
    .desc: In this function, each process i gathers data items
{
    mpi_errno = MPIR_Neighbor_allgather(sendbuf, sendcount, sendtype, recvbuf,
                                        recvcount, recvtype, comm_ptr);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Neighbor_allgatherv:
    .desc: The vector variant of MPI_Neighbor_allgather.
{
    mpi_errno = MPIR_Neighbor_allgatherv(sendbuf, sendcount, sendtype, recvbuf,
                                         recvcounts, displs, recvtype, comm_ptr);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Neighbor_alltoall:
    .desc: In this function, each process i receives data items
{
    mpi_errno = MPIR_Neighbor_alltoall(sendbuf, sendcount, sendtype, recvbuf,
                                       recvcount, recvtype, comm_ptr);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Neighbor_alltoallv:
    .desc: The vector variant of MPI_Neighbor_alltoall allows
{
    mpi_errno = MPIR_Neighbor_alltoallv(sendbuf, sendcounts, sdispls, sendtype,
                                        recvbuf, recvcounts, rdispls, recvtype, comm_ptr);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Neighbor_alltoallw:
    .desc: Like MPI_Neighbor_alltoallv but it allows one to send
{
    mpi_errno = MPIR_Neighbor_alltoallw_impl(sendbuf, sendcounts, sdispls,
                                             sendtypes, recvbuf, recvcounts,
                                             rdispls, recvtypes, comm_ptr);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Reduce:
    .desc: Reduces values on all processes to a single value
    .extra: collops

MPI_Reduce_local:
    .desc: Applies a reduction operator to local arguments.
    .extra: collops
{
    mpi_errno = MPIR_Reduce_local(inbuf, inoutbuf, count, datatype, op);
    if (mpi_errno) {
        goto fn_fail;
    }
}

MPI_Reduce_scatter:
    .desc: Combines values and scatters the results
    .extra: collops

MPI_Reduce_scatter_block:
    .desc: Combines values and scatters the results
    .extra: collops

MPI_Scan:
    .desc: Computes the scan (partial reductions) of data on a collection of
    .extra: collops, errtest_comm_intra

MPI_Scatter:
    .desc: Sends data from one process to all other processes in a

MPI_Scatterv:
    .desc: Scatters a buffer in parts to all processes in a communicator
