# Conditional XFAIL settings
#
# Syntax (similar to a cron file):
#   [jobname] [compiler] [jenkins_configure] [netmod] [queue] [regex] [ticket reference] [testlist]
#   Note that the [jobname] allows partial matches (see examples). Other
#   conditions only allow exact matches.
#   The actual allowed combinations are depending on the Jenkins job. For
#   example,
#     ofi * * tcp * /.../
#   will have no effect since none of the ofi jobs has tcp netmod in the
#   configuration.
#
# Examples:
#   tcp gnu debug * * /.../
#   This will apply the XFAIL when the job is "mpich-main-tcp" or
#   "mpich-review-tcp", the compiler is "gnu", and the jenkins_configure is
#   "debug".
#
#   main-ubuntu * * * ubuntu32 /.../
#   This will apply the set the XFAIL when the job is "mpich-main-ubuntu" and
#   the running queue is "ubuntu32".
#
# For each build, set_xfail.py will summarize all applied XFAILS in the
# console log.
#

################################################################################

# xfail ch4 bugs
* * * ch4:ofi *         /^idup_comm_gen/ xfail=ticket3794        threads/comm/testlist
* * * ch4:ofi *         /^idup_nb/       xfail=ticket3794        threads/comm/testlist
* * * ch4:ucx *         /^idup_comm_gen/ xfail=ticket3794        threads/comm/testlist
* * * ch4:ucx *         /^idup_nb/       xfail=ticket3794        threads/comm/testlist
################################################################################
# misc special build
* * nofast * *          /^large_acc_flush_local/ xfail=issue4663 rma/testlist
################################################################################
* * * * osx             /^throwtest/            xfail=ticket0   errors/cxx/errhan/testlist
* * * * osx             /^commerrx/             xfail=ticket0   errors/cxx/errhan/testlist
* * * * osx             /^fileerrretx/          xfail=ticket0   errors/cxx/io/testlist
* * * * osx             /^throwtestfilex/       xfail=ticket0   errors/cxx/io/testlist
* gnu debug ch3:tcp osx /^namepubx/             xfail=issue3506 cxx/spawn/testlist
################################################################################
# xfail large count tests on 32 bit architectures (cannot allocate such large memory)
* * * * freebsd32       /^getfence1 [0-9]* arg=-type=.* arg=-count=16000000/     xfail=ticket0   rma/testlist.dtp
* * * * freebsd32       /^putfence1 [0-9]* arg=-type=.* arg=-count=16000000/     xfail=ticket0   rma/testlist.dtp
* * * * ubuntu32        /^getfence1 [0-9]* arg=-type=.* arg=-count=16000000/     xfail=ticket0   rma/testlist.dtp
* * * * ubuntu32        /^putfence1 [0-9]* arg=-type=.* arg=-count=16000000/     xfail=ticket0   rma/testlist.dtp

# xpmem job is running on yuzu and 10 ppn nonblocking3 is too stressful
* * * * xpmem           /^nonblocking3 10/                                      xfail=ticket0   coll/testlist

# intercomm abort test are expected to fail since MPI_Finalize will try to perform Allreduce on all process (including the aborted ones)
* * * * *               /^intercomm_abort/       xfail=ticket0   errors/comm/testlist
# asan glitches with ucx for large buffer (when greater than ~1GB)
* * asan ch4:ucx *      /^.*262144\|65530\|16000000.*/  xfail=ticket0   coll/testlist.dtp
* * asan ch4:ucx *      /^.*262144\|65530\|16000000.*/  xfail=ticket0   pt2pt/testlist.dtp
* * asan ch4:ucx *      /^.*262144\|65530\|16000000.*/  xfail=ticket0   rma/testlist.dtp
# Bug - Github Issue https://github.com/pmodels/mpich/issues/3618
* * * ch4:ucx *         /^darray_pack/   xfail=ticket0   datatype/testlist
# UCX requires a larger MPI_MAX_PORT_NAME
* * * ch4:ucx *         /^selfconaccx/   xfail=ticket0   cxx/spawn/testlist
* * * ch4:ucx *         /^connaccf/      xfail=ticket0   f77/spawn/testlist
* * * ch4:ucx *         /^connaccf90/    xfail=ticket0   f90/spawn/testlist
* * * ch4:ucx *         /^connaccf90/    xfail=ticket0   f08/spawn/testlist

# multi-threading tests failing for ch3 freebsd64
* * * ch3:tcp *         /^mt_iprobe_isendrecv/          xfail=issue4258         threads/pt2pt/testlist
* * * ch3:tcp *         /^mt_improbe_isendrecv/         xfail=issue4258         threads/pt2pt/testlist
# ch3:sock sporadicly TIMEOUT on these async tests
* * async ch3:sock *    /^nonblocking3 /                  xfail=issue6264        coll/testlist
* * async ch3:sock *    /^i?write(sh|ord|ordbe|all|allbe)f /        xfail=issue6264        f77/io/testlist
* * async ch3:sock *    /^i?write(sh|ord|ordbe|all|allbe)f90 /      xfail=issue6264        f90/io/testlist
# dup_leak_test suffers from mutex unfairness issue under load for ch4:ofi
* * * ch4:ofi *         /^dup_leak_test .*iter=12345.*/  xfail=issue4595        threads/comm/testlist
# more mutex unfairness on freebsd64. note: check these after upgrading freebsd64 hardware
* * * ch4:ofi freebsd64 /^dup_leak_test .*iter=1234.*/   xfail=issue4595        threads/comm/testlist
* * * ch4:ofi freebsd64 /^alltoall/                      xfail=issue4595        threads/pt2pt/testlist
# freebsd failures
* * debug ch3:tcp freebsd64     /^comm_create_group_threads/     xfail=issue4372        threads/comm/testlist

# timeout due to lack of passive progress
* * vci ch4:ofi *       /^rma_contig.*iter=10000/        xfail=issue5565        rma/testlist

# multiple vci + too many process may run out of endpoints or descriptors
* * vci ch4:ofi *       /^darray_pack 72/                xfail=pr5998           datatype/testlist
* * vci ch4:ofi *       /^spawn_rootargs 10/             xfail=pr5998           spawn/testlist

# The cuda wait kernel may deadlock with progress thread
* * * ch4:ofi *         /^stream .*-progress-thread/    xfail=ticket0           impls/mpich/cuda/testlist

# Sunf90 forbid passing cray pointer as integer
* solstudio * * *       /^allocmemf90/          xfail=ticket0      f90/ext/testlist

# Job-sepecific xfails
# Our Arm servers are too slow for some tests
mpich-.*-arm.* * * * * /^sendflood /          xfail=ticket0       pt2pt/testlist
mpich-.*-arm.* * * * * /^nonblocking3 /       xfail=ticket0       coll/testlist
mpich-.*-arm.* * * * * /^alltoall /           xfail=ticket0       threads/pt2pt/testlist
mpich-.*-arm.* * * * * /^reduce 10/           xfail=ticket0       coll/testlist.collalgo

# pmix doesn't work well with ch3 under oversubscription, ref. PR5984
* * pmix ch3:.* *       /^ic2 33/               xfail=ticket0       comm/testlist
* * pmix ch3:.* *       /^darray_pack 72/       xfail=ticket0       datatype/testlist
# MPI_Abort a sub group is not fully working
* * * * *               /^(inter|sub)comm_abort/   xfail=ticket6634    errors/comm/testlist

# The pipelined_tree bcast algorithm may flood unexpected messages to children processes
* * * * *               /^bcasttest 10 .*ALGORITHM=pipelined_tree/  xfail=ticket4474 coll/testlist.collalgo

# IPC read bcast, alltoall, allgather and allgatherv fail as MPL_gpu_imemcpy is not implemented in CUDA
# https://github.com/pmodels/mpich/issues/6657
* * * * *               /^bcast_gpu/   xfail=ticket6657    coll/testlist.gpu
* * * * *               /^alltoall_gpu/   xfail=ticket6657    coll/testlist.gpu
* * * * *               /^allgather_gpu/   xfail=ticket6657    coll/testlist.gpu
* * * * *               /^allgatherv_gpu/   xfail=ticket6657    coll/testlist.gpu

# GPU pipelining requires provider cq_data_sz > 8 which isn't met by psm3
* * * ch4:ofi *         /^.*MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE=1.*/    xfail=ticket0           pt2pt/testlist.gpu
